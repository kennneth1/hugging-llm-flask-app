from transformers import AutoModelForCausalLM, AutoTokenizer
from config import (
    NUM_BEAMS,
    NO_REPEAT_NGRAM_SIZE,
    MAX_LENGTH,
)

def load_model(model_name):
    # Download the model and tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    return model, tokenizer

def chat(input_message, model_name):
    # Load model and tokenizer 
    model, tokenizer = load_model(model_name)
    # Tokenize the input message
    input_ids = tokenizer.encode(input_message, return_tensors="pt")
    # Generate a response from the model
    output = model.generate(input_ids, max_length=MAX_LENGTH, num_beams=NUM_BEAMS, no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE)
    # Decode the generated response
    response = tokenizer.decode(output[0], skip_special_tokens=True)

    # Print the response as JSON
    print({"response": response})

    # Return the response as a dictionary
    return {"response": response}

def clean_response(response, prompt):
    """
    Clean responses generated by LLM
    """
    cleaned_response = response.replace(prompt, "").strip()

    return cleaned_response
